{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4da7a59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_train = r\"C:\\Users\\maxin\\Documents\\Master\\Year 1\\Period 5\\Data Mining\\Assignment 2\\training_set_VU_DM.csv\"\n",
    "df_train = pd.read_csv(file_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30357244",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()\n",
    "df_train.dtypes\n",
    "df = df_train.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f29fef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = (df_train.isnull().sum()/len(df_train))*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1cd3ce3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "srch_id                         0.000000\n",
       "date_time                       0.000000\n",
       "site_id                         0.000000\n",
       "visitor_location_country_id     0.000000\n",
       "visitor_hist_starrating        94.920364\n",
       "visitor_hist_adr_usd           94.897735\n",
       "prop_country_id                 0.000000\n",
       "prop_id                         0.000000\n",
       "prop_starrating                 0.000000\n",
       "prop_review_score               0.148517\n",
       "prop_brand_bool                 0.000000\n",
       "prop_location_score1            0.000000\n",
       "prop_location_score2           21.990151\n",
       "prop_log_historical_price       0.000000\n",
       "position                        0.000000\n",
       "price_usd                       0.000000\n",
       "promotion_flag                  0.000000\n",
       "srch_destination_id             0.000000\n",
       "srch_length_of_stay             0.000000\n",
       "srch_booking_window             0.000000\n",
       "srch_adults_count               0.000000\n",
       "srch_children_count             0.000000\n",
       "srch_room_count                 0.000000\n",
       "srch_saturday_night_bool        0.000000\n",
       "srch_query_affinity_score      93.598552\n",
       "orig_destination_distance      32.425766\n",
       "random_bool                     0.000000\n",
       "comp1_rate                     97.581250\n",
       "comp1_inv                      97.387053\n",
       "comp1_rate_percent_diff        98.095353\n",
       "comp2_rate                     59.166392\n",
       "comp2_inv                      57.036710\n",
       "comp2_rate_percent_diff        88.781786\n",
       "comp3_rate                     69.056462\n",
       "comp3_inv                      66.702814\n",
       "comp3_rate_percent_diff        90.464625\n",
       "comp4_rate                     93.800797\n",
       "comp4_inv                      93.069001\n",
       "comp4_rate_percent_diff        97.356256\n",
       "comp5_rate                     55.179155\n",
       "comp5_inv                      52.403089\n",
       "comp5_rate_percent_diff        83.036706\n",
       "comp6_rate                     95.156511\n",
       "comp6_inv                      94.736633\n",
       "comp6_rate_percent_diff        98.060362\n",
       "comp7_rate                     93.640058\n",
       "comp7_inv                      92.811677\n",
       "comp7_rate_percent_diff        97.206428\n",
       "comp8_rate                     61.344900\n",
       "comp8_inv                      59.916016\n",
       "comp8_rate_percent_diff        87.602118\n",
       "click_bool                      0.000000\n",
       "gross_bookings_usd             97.208949\n",
       "booking_bool                    0.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83e6c616",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>srch_id</th>\n",
       "      <th>date_time</th>\n",
       "      <th>site_id</th>\n",
       "      <th>visitor_location_country_id</th>\n",
       "      <th>visitor_hist_starrating</th>\n",
       "      <th>visitor_hist_adr_usd</th>\n",
       "      <th>prop_country_id</th>\n",
       "      <th>prop_id</th>\n",
       "      <th>prop_starrating</th>\n",
       "      <th>prop_review_score</th>\n",
       "      <th>...</th>\n",
       "      <th>comp6_rate_percent_diff</th>\n",
       "      <th>comp7_rate</th>\n",
       "      <th>comp7_inv</th>\n",
       "      <th>comp7_rate_percent_diff</th>\n",
       "      <th>comp8_rate</th>\n",
       "      <th>comp8_inv</th>\n",
       "      <th>comp8_rate_percent_diff</th>\n",
       "      <th>click_bool</th>\n",
       "      <th>gross_bookings_usd</th>\n",
       "      <th>booking_bool</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2013-04-04 08:32:15</td>\n",
       "      <td>12</td>\n",
       "      <td>187</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>219</td>\n",
       "      <td>893</td>\n",
       "      <td>3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2013-04-04 08:32:15</td>\n",
       "      <td>12</td>\n",
       "      <td>187</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>219</td>\n",
       "      <td>10404</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2013-04-04 08:32:15</td>\n",
       "      <td>12</td>\n",
       "      <td>187</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>219</td>\n",
       "      <td>21315</td>\n",
       "      <td>3</td>\n",
       "      <td>4.5</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2013-04-04 08:32:15</td>\n",
       "      <td>12</td>\n",
       "      <td>187</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>219</td>\n",
       "      <td>27348</td>\n",
       "      <td>2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2013-04-04 08:32:15</td>\n",
       "      <td>12</td>\n",
       "      <td>187</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>219</td>\n",
       "      <td>29604</td>\n",
       "      <td>4</td>\n",
       "      <td>3.5</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 54 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   srch_id            date_time  site_id  visitor_location_country_id  \\\n",
       "0        1  2013-04-04 08:32:15       12                          187   \n",
       "1        1  2013-04-04 08:32:15       12                          187   \n",
       "2        1  2013-04-04 08:32:15       12                          187   \n",
       "3        1  2013-04-04 08:32:15       12                          187   \n",
       "4        1  2013-04-04 08:32:15       12                          187   \n",
       "\n",
       "   visitor_hist_starrating  visitor_hist_adr_usd  prop_country_id  prop_id  \\\n",
       "0                      NaN                   NaN              219      893   \n",
       "1                      NaN                   NaN              219    10404   \n",
       "2                      NaN                   NaN              219    21315   \n",
       "3                      NaN                   NaN              219    27348   \n",
       "4                      NaN                   NaN              219    29604   \n",
       "\n",
       "   prop_starrating  prop_review_score  ...  comp6_rate_percent_diff  \\\n",
       "0                3                3.5  ...                      2.0   \n",
       "1                4                4.0  ...                      2.0   \n",
       "2                3                4.5  ...                      2.0   \n",
       "3                2                4.0  ...                      2.0   \n",
       "4                4                3.5  ...                      2.0   \n",
       "\n",
       "   comp7_rate  comp7_inv  comp7_rate_percent_diff  comp8_rate  comp8_inv  \\\n",
       "0         2.0        2.0                      2.0         0.0        0.0   \n",
       "1         2.0        2.0                      2.0         0.0        0.0   \n",
       "2         2.0        2.0                      2.0         0.0        0.0   \n",
       "3         2.0        2.0                      2.0        -1.0        0.0   \n",
       "4         2.0        2.0                      2.0         0.0        0.0   \n",
       "\n",
       "   comp8_rate_percent_diff  click_bool  gross_bookings_usd  booking_bool  \n",
       "0                      2.0           0                 NaN             0  \n",
       "1                      2.0           0                 NaN             0  \n",
       "2                      2.0           0                 NaN             0  \n",
       "3                      5.0           0                 NaN             0  \n",
       "4                      2.0           0                 NaN             0  \n",
       "\n",
       "[5 rows x 54 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Impute a new category 2, that indicates unknown values\n",
    "#This is done for all competitor values\n",
    "df_train[['comp1_rate', 'comp1_inv', 'comp1_rate_percent_diff', 'comp2_rate', 'comp2_inv', 'comp2_rate_percent_diff', 'comp3_rate', 'comp3_inv', 'comp3_rate_percent_diff', 'comp4_rate', 'comp4_inv', 'comp4_rate_percent_diff', 'comp5_rate', 'comp5_inv', 'comp5_rate_percent_diff', 'comp6_rate', 'comp6_inv', 'comp6_rate_percent_diff', 'comp7_rate', 'comp7_inv','comp7_rate_percent_diff', 'comp8_rate', 'comp8_inv', 'comp8_rate_percent_diff']] = df_train[['comp1_rate', 'comp1_inv', 'comp1_rate_percent_diff', 'comp2_rate', 'comp2_inv', 'comp2_rate_percent_diff', 'comp3_rate', 'comp3_inv', 'comp3_rate_percent_diff', 'comp4_rate', 'comp4_inv', 'comp4_rate_percent_diff', 'comp5_rate', 'comp5_inv', 'comp5_rate_percent_diff', 'comp6_rate', 'comp6_inv', 'comp6_rate_percent_diff', 'comp7_rate', 'comp7_inv','comp7_rate_percent_diff', 'comp8_rate', 'comp8_inv', 'comp8_rate_percent_diff']].fillna(2.0)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75199e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Impute price_usd for missing values in gross_bookings_usd\n",
    "# df_train['gross_bookings_usd'] = df_train['gross_bookings_usd'].fillna(df_train['price_usd'])\n",
    "# df_train['gross_bookings_usd'].isnull().sum()\n",
    "\n",
    "#Delete column gross_bookings_usd\n",
    "df_train = df_train.drop(['gross_bookings_usd'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80a568e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "visitor_hist_starrating    0\n",
       "visitor_hist_adr_usd       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Impute a new category -1 which means no history for previous starratings and price per night\n",
    "df_train[['visitor_hist_starrating', 'visitor_hist_adr_usd']] = df_train[['visitor_hist_starrating', 'visitor_hist_adr_usd']].fillna(-1)\n",
    "df_train[['visitor_hist_starrating', 'visitor_hist_adr_usd']].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55ea70e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>srch_id</th>\n",
       "      <th>date_time</th>\n",
       "      <th>site_id</th>\n",
       "      <th>visitor_location_country_id</th>\n",
       "      <th>visitor_hist_starrating</th>\n",
       "      <th>visitor_hist_adr_usd</th>\n",
       "      <th>prop_country_id</th>\n",
       "      <th>prop_id</th>\n",
       "      <th>prop_starrating</th>\n",
       "      <th>prop_review_score</th>\n",
       "      <th>...</th>\n",
       "      <th>comp6_inv</th>\n",
       "      <th>comp6_rate_percent_diff</th>\n",
       "      <th>comp7_rate</th>\n",
       "      <th>comp7_inv</th>\n",
       "      <th>comp7_rate_percent_diff</th>\n",
       "      <th>comp8_rate</th>\n",
       "      <th>comp8_inv</th>\n",
       "      <th>comp8_rate_percent_diff</th>\n",
       "      <th>click_bool</th>\n",
       "      <th>booking_bool</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2013-04-04 08:32:15</td>\n",
       "      <td>12</td>\n",
       "      <td>187</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>219</td>\n",
       "      <td>893</td>\n",
       "      <td>3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2013-04-04 08:32:15</td>\n",
       "      <td>12</td>\n",
       "      <td>187</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>219</td>\n",
       "      <td>10404</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2013-04-04 08:32:15</td>\n",
       "      <td>12</td>\n",
       "      <td>187</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>219</td>\n",
       "      <td>21315</td>\n",
       "      <td>3</td>\n",
       "      <td>4.5</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2013-04-04 08:32:15</td>\n",
       "      <td>12</td>\n",
       "      <td>187</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>219</td>\n",
       "      <td>27348</td>\n",
       "      <td>2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2013-04-04 08:32:15</td>\n",
       "      <td>12</td>\n",
       "      <td>187</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>219</td>\n",
       "      <td>29604</td>\n",
       "      <td>4</td>\n",
       "      <td>3.5</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 52 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   srch_id            date_time  site_id  visitor_location_country_id  \\\n",
       "0        1  2013-04-04 08:32:15       12                          187   \n",
       "1        1  2013-04-04 08:32:15       12                          187   \n",
       "2        1  2013-04-04 08:32:15       12                          187   \n",
       "3        1  2013-04-04 08:32:15       12                          187   \n",
       "4        1  2013-04-04 08:32:15       12                          187   \n",
       "\n",
       "   visitor_hist_starrating  visitor_hist_adr_usd  prop_country_id  prop_id  \\\n",
       "0                     -1.0                  -1.0              219      893   \n",
       "1                     -1.0                  -1.0              219    10404   \n",
       "2                     -1.0                  -1.0              219    21315   \n",
       "3                     -1.0                  -1.0              219    27348   \n",
       "4                     -1.0                  -1.0              219    29604   \n",
       "\n",
       "   prop_starrating  prop_review_score  ...  comp6_inv  \\\n",
       "0                3                3.5  ...        2.0   \n",
       "1                4                4.0  ...        2.0   \n",
       "2                3                4.5  ...        2.0   \n",
       "3                2                4.0  ...        2.0   \n",
       "4                4                3.5  ...        2.0   \n",
       "\n",
       "   comp6_rate_percent_diff  comp7_rate  comp7_inv  comp7_rate_percent_diff  \\\n",
       "0                      2.0         2.0        2.0                      2.0   \n",
       "1                      2.0         2.0        2.0                      2.0   \n",
       "2                      2.0         2.0        2.0                      2.0   \n",
       "3                      2.0         2.0        2.0                      2.0   \n",
       "4                      2.0         2.0        2.0                      2.0   \n",
       "\n",
       "   comp8_rate  comp8_inv  comp8_rate_percent_diff  click_bool  booking_bool  \n",
       "0         0.0        0.0                      2.0           0             0  \n",
       "1         0.0        0.0                      2.0           0             0  \n",
       "2         0.0        0.0                      2.0           0             0  \n",
       "3        -1.0        0.0                      5.0           0             0  \n",
       "4         0.0        0.0                      2.0           0             0  \n",
       "\n",
       "[5 rows x 52 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Delete the column srch_query_affinity_score because 93.6% of the hotels did not register any searches (lot of missing data)\n",
    "df_train = df_train.drop(['srch_query_affinity_score'], axis = 1)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "353b3579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   srch_destination_id  prop_country_id  orig_destination_distance\n",
      "0                23246              219                        NaN\n",
      "1                23246              219                        NaN\n",
      "2                23246              219                        NaN\n",
      "3                23246              219                        NaN\n",
      "4                23246              219                        NaN\n"
     ]
    }
   ],
   "source": [
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "    print(df_train[['srch_destination_id', 'prop_country_id', 'orig_destination_distance']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f5fa0658-4ddd-4bcf-b2a5-52ed2574bf6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   prop_location_score1  prop_location_score2\n",
      "0                  2.83                0.0438\n",
      "1                  2.20                0.0149\n",
      "2                  2.20                0.0245\n",
      "3                  2.83                0.0125\n",
      "4                  2.64                0.1241\n"
     ]
    }
   ],
   "source": [
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "    print(df_train[['prop_location_score1', 'prop_location_score2']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a9714f96-1f65-48ce-b557-8de15d4ef0d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "# dest = df_train[ 'prop_country_id'].values\n",
    "# np.unique(dest)\n",
    "\n",
    "\n",
    "# pd.set_option('display.max_colwidth', None, 'display.max_rows', 20)\n",
    "# # df_train['prop_location_score2'] = df_train['prop_location_score2'].fillna(df_train.groupby('prop_country_id')['prop_location_score2'].mean())\n",
    "\n",
    "# df_train[df_train['prop_country_id']==219].head(30)\n",
    "\n",
    "means = df_train.groupby('prop_country_id')['prop_location_score2'].mean().to_dict()\n",
    "for index, value in enumerate(df_train['prop_location_score2']):\n",
    "    if np.isnan(value):\n",
    "        country  = df_train['prop_country_id'][index]\n",
    "        for key, value in means.items():\n",
    "            if key == country:\n",
    "                df_train['prop_location_score2'][index] == value\n",
    "\n",
    "# df_train['prop_location_score2'].isnull().sum()\n",
    "k = df_train[df_train['prop_country_id']==29]\n",
    "k['prop_location_score2'].mean()\n",
    "# df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ba294934-02e1-464a-9041-b72446a556f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    219\n",
       "2    219\n",
       "3    219\n",
       "4    219\n",
       "5    219\n",
       "6    219\n",
       "7    219\n",
       "8    219\n",
       "9    219\n",
       "Name: prop_country_id, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['prop_country_id'][1:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f69340e-3105-4886-af55-2dd8f1e14c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fill the missing value with the value of the row above it\n",
    "# df_train['prop_location_score2'] = df_train['prop_location_score2'].fillna(method = 'ffill')\n",
    "# df_train['prop_location_score2'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3fad78-0ed5-4089-bbe6-91a0e276c4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(df['prop_location_score2'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99480db-4fcf-474c-84d7-635535645c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df_train['prop_location_score2'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbf25f0-3588-4efb-bf92-520e2939cabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train = df_train.drop(['prop_review_score'], axis = 1) \n",
    "df_train = df_train.dropna(subset=['prop_review_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7091d41-d0bc-44e8-a3f1-2f87de9fc491",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = (df_train.isnull().sum()/len(df_train))*100\n",
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c2b35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For the distance we are going to predict the distance by using the search location of the user and hotel location\n",
    "#For the missing values the predicted distance will be imputed\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn import metrics\n",
    "\n",
    "# lr = LogisticRegression()\n",
    "\n",
    "# testdf = df_train[df_train['orig_destination_distance'].isnull()==True]\n",
    "# traindf = df_train[df_train['orig_destination_distance'].isnull()==False]\n",
    "\n",
    "# y = traindf['orig_destination_distance']\n",
    "# X = traindf[['srch_destination_id', 'prop_country_id']]\n",
    "\n",
    "# #Splitting train data into validation and train set\n",
    "# X_train, X_val,y_train,y_val = train_test_split(X,y,test_size=0.3)\n",
    "\n",
    "# X_train\n",
    "# lr.fit(X_train,y_train)\n",
    "\n",
    "# # y_test = testdf['orig_destination_distance']\n",
    "# # testdf = testdf[['srch_destination_id', 'prop_country_id']]\n",
    "\n",
    "# LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
    "#                    intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
    "#                    multi_class='auto', n_jobs=None, penalty='l2',\n",
    "#                    random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
    "#                    warm_start=False)\n",
    "\n",
    "# pred = lr.predict(X_val)\n",
    "\n",
    "\n",
    "\n",
    "# testdf['orig_destination_distance']= pred\n",
    "# reg.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a14a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.impute import KNNImputer\n",
    "# imputer = KNNImputer(n_neighbors=2, weights=\"uniform\")\n",
    "# imputer.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b4f51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "testdf = df_train[df_train['orig_destination_distance'].isnull()==True]\n",
    "traindf = df_train[df_train['orig_destination_distance'].isnull()==False]\n",
    "\n",
    "y = traindf['orig_destination_distance']\n",
    "X = traindf[['srch_destination_id', 'prop_country_id']]\n",
    "\n",
    "#Splitting train data into validation and train set\n",
    "X_train, X_val,y_train,y_val = train_test_split(X,y,test_size=0.3)\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "x_train_scaled = scaler.fit_transform(X_train)\n",
    "X_train = pd.DataFrame(x_train_scaled)\n",
    "\n",
    "x_test_scaled = scaler.fit_transform(X_val)\n",
    "X_val = pd.DataFrame(x_test_scaled)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e9670b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #import required packages\n",
    "# from sklearn import neighbors\n",
    "# from sklearn.metrics import mean_squared_error \n",
    "# from math import sqrt\n",
    "# import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "\n",
    "# rmse_val = [] #to store rmse values for different k\n",
    "# for K in range(20):\n",
    "#     K = K+1\n",
    "#     model = neighbors.KNeighborsRegressor(n_neighbors = K)\n",
    "\n",
    "#     model.fit(X_train, y_train)  #fit the model\n",
    "#     pred=model.predict(X_val) #make prediction on test set\n",
    "#     error = sqrt(mean_squared_error(y_val,pred)) #calculate rmse\n",
    "#     rmse_val.append(error) #store rmse values\n",
    "#     print('RMSE value for k= ' , K , 'is:', error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ebbcf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import GridSearchCV\n",
    "# from sklearn import neighbors\n",
    "\n",
    "# params = {'n_neighbors':[9, 11]}\n",
    "\n",
    "# knn = neighbors.KNeighborsRegressor()\n",
    "\n",
    "# model = GridSearchCV(knn, params, cv=5)\n",
    "# model.fit(X_train,y_train)\n",
    "# model.best_params_\n",
    "\n",
    "# #n_neighbors = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129283ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make dummy variables of the features that have information regarding competitors\n",
    "# comp1_rate\n",
    "# dummy1 = pd.get_dummies(df_train['comp1_rate'])\n",
    "# df_dummy1 = dummy1.rename(columns = {(-1): \"comp1_rate_-1\", 0: \"comp1_rate_0\", 1: \"comp1_rate_1\", 2: \"comp1_rate_2\"})\n",
    "# # df_train = pd.concat([df_train, df_dummy1], axis = 1)\n",
    "\n",
    "# # #comp1_inv                      \n",
    "# dummy2 = pd.get_dummies(df_train['comp1_inv'])\n",
    "# df_dummy2 = dummy2.rename(columns = {(-1): \"comp1_inv_-1\", 0: \"comp1_inv_0\", 1: \"comp1_inv_1\", 2: \"comp1_inv_2\"})\n",
    "# # df_train = pd.concat([df_train, df_dummy2], axis = 1)\n",
    "\n",
    "# # # comp1_rate_percent_diff        \n",
    "# dummy3 = pd.get_dummies(df_train['comp1_rate_percent_diff'])\n",
    "# df_dummy3 = dummy3.rename(columns = {(-1): \"comp1_rate_percent_diff_-1\", 0: \"comp1_rate_percent_diff_0\", 1: \"comp1_rate_percent_diff_1\", 2: \"comp1_rate_percent_diff_2\"})\n",
    "# # df_train = pd.concat([df_train, df_dummy3], axis = 1)\n",
    "\n",
    "# # # comp2_rate                     \n",
    "# dummy4 = pd.get_dummies(df_train['comp2_rate'])\n",
    "# df_dummy4 = dummy4.rename(columns = {(-1): \"comp2_rate_-1\", 0: \"comp2_rate_0\", 1: \"comp2_rate_1\", 2: \"comp2_rate_2\"})\n",
    "# # df_train = pd.concat([df_train, df_dummy4], axis = 1)\n",
    "\n",
    "# df_train1 = pd.concat([df_train, df_dummy1], axis = 1)\n",
    "# df_train2 = pd.concat([df_train1, df_dummy2], axis = 1)\n",
    "# df_train3 = pd.concat([df_train2, df_dummy3], axis = 1)\n",
    "# df_train4 = pd.concat([df_train3, df_dummy4], axis = 1)\n",
    "\n",
    "\n",
    "# # comp2_inv                      \n",
    "# dummy5 = pd.get_dummies(df_train['comp2_inv'])\n",
    "# df_dummy5 = dummy5.rename(columns = {(-1): \"comp2_inv_-1\", 0: \"comp2_inv_0\", 1: \"comp2_inv_1\", 2: \"comp2_inv_2\"})\n",
    "# df_train = pd.concat([df_train, df_dummy5], axis = 1)\n",
    "\n",
    "# # comp2_rate_percent_diff        \n",
    "# dummy6 = pd.get_dummies(df_train['comp2_rate_percent_diff'])\n",
    "# df_dummy6 = dummy6.rename(columns = {(-1): \"comp2_rate_percent_diff_-1\", 0: \"comp2_rate_percent_diff_0\", 1: \"comp2_rate_percent_diff_1\", 2: \"comp2_rate_percent_diff_2\"})\n",
    "# df_train = pd.concat([df_train, df_dummy6], axis = 1)\n",
    "\n",
    "# # comp3_rate                     \n",
    "# dummy7 = pd.get_dummies(df_train['comp3_rate'])\n",
    "# df_dummy7 = dummy7.rename(columns = {(-1): \"comp3_rate_-1\", 0: \"comp3_rate_0\", 1: \"comp3_rate_1\", 2: \"comp3_rate_2\"})\n",
    "# df_train = pd.concat([df_train, df_dummy7], axis = 1)\n",
    "\n",
    "# # comp3_inv                      \n",
    "# dummy8 = pd.get_dummies(df_train['comp3_inv'])\n",
    "# df_dummy8 = dummy8.rename(columns = {(-1): \"comp3_inv_-1\", 0: \"comp3_inv_0\", 1: \"comp3_inv_1\", 2: \"comp3_inv_2\"})\n",
    "# df_train = pd.concat([df_train, df_dummy8], axis = 1)\n",
    "\n",
    "# # comp3_rate_percent_diff        \n",
    "# dummy9 = pd.get_dummies(df_train['comp3_inv'])\n",
    "# df_dummy9 = dummy9.rename(columns = {(-1): \"comp3_rate_percent_diff_-1\", 0: \"comp3_rate_percent_diff_0\", 1: \"comp3_rate_percent_diff_1\", 2: \"comp3_rate_percent_diff_2\"})\n",
    "# df_train = pd.concat([df_train, df_dummy9], axis = 1)\n",
    "\n",
    "# # comp4_rate                     \n",
    "# dummy9 = pd.get_dummies(df_train['comp4_rate'])\n",
    "# df_dummy9 = dummy9.rename(columns = {(-1): \"comp4_rate_-1\", 0: \"comp4_rate_0\", 1: \"comp4_rate_1\", 2: \"comp4_rate_2\"})\n",
    "# df_train = pd.concat([df_train, df_dummy9], axis = 1)\n",
    "\n",
    "# # comp4_inv                      \n",
    "# dummy10 = pd.get_dummies(df_train['comp4_inv'])\n",
    "# df_dummy10 = dummy10.rename(columns = {(-1): \"comp4_inv_-1\", 0: \"comp4_inv_0\", 1: \"comp4_inv_1\", 2: \"comp4_inv_2\"})\n",
    "# df_train = pd.concat([df_train, df_dummy10], axis = 1)\n",
    "\n",
    "# # comp4_rate_percent_diff        \n",
    "# dummy11 = pd.get_dummies(df_train['comp4_rate_percent_diff'])\n",
    "# df_dummy11 = dummy11.rename(columns = {(-1): \"comp4_rate_percent_diff_-1\", 0: \"comp4_rate_percent_diff_0\", 1: \"comp4_rate_percent_diff_1\", 2: \"comp4_rate_percent_diff_2\"})\n",
    "# df_train = pd.concat([df_train, df_dummy11], axis = 1)\n",
    "\n",
    "# # comp5_rate                     \n",
    "# dummy12 = pd.get_dummies(df_train['comp5_rate'])\n",
    "# df_dummy12 = dummy12.rename(columns = {(-1): \"comp5_rate_-1\", 0: \"comp5_rate_0\", 1: \"comp5_rate_1\", 2: \"comp5_rate_2\"})\n",
    "# df_train = pd.concat([df_train, df_dummy12], axis = 1)\n",
    "\n",
    "# # comp5_inv                      \n",
    "# dummy13 = pd.get_dummies(df_train['comp5_inv'])\n",
    "# df_dummy13 = dummy13.rename(columns = {(-1): \"comp5_inv_-1\", 0: \"comp5_inv_0\", 1: \"comp5_inv_1\", 2: \"comp5_inv_2\"})\n",
    "# df_train = pd.concat([df_train, df_dummy13], axis = 1)\n",
    "\n",
    "# # comp5_rate_percent_diff    \n",
    "# dummy14 = pd.get_dummies(df_train['comp5_rate_percent_diff'])\n",
    "# df_dummy14 = dummy14.rename(columns = {(-1): \"comp5_rate_percent_diff_-1\", 0: \"comp5_rate_percent_diff_0\", 1: \"comp5_rate_percent_diff_1\", 2: \"comp5_rate_percent_diff_2\"})\n",
    "# df_train = pd.concat([df_train, df_dummy14], axis = 1)\n",
    "\n",
    "# #comp6_rate   \n",
    "# dummy15 = pd.get_dummies(df_train['comp6_rate'])\n",
    "# df_dummy15 = dummy15.rename(columns = {(-1): \"comp6_rate_-1\", 0: \"comp6_rate_0\", 1: \"comp6_rate_1\", 2: \"comp6_rate_2\"})\n",
    "# df_train = pd.concat([df_train, df_dummy15], axis = 1)\n",
    "\n",
    "# #comp6_inv  \n",
    "# dummy16 = pd.get_dummies(df_train['comp6_inv'])\n",
    "# df_dummy16 = dummy16.rename(columns = {(-1): \"comp6_inv_-1\", 0: \"comp6_inv_0\", 1: \"comp6_inv_1\", 2: \"comp6_inv_2\"})\n",
    "# df_train = pd.concat([df_train, df_dummy16], axis = 1)\n",
    "\n",
    "# #comp6_rate_percent_diff   \n",
    "# dummy17 = pd.get_dummies(df_train['comp6_rate_percent_diff'])\n",
    "# df_dummy17 = dummy17.rename(columns = {(-1): \"comp6_rate_percent_diff_-1\", 0: \"comp6_rate_percent_diff_0\", 1: \"comp6_rate_percent_diff_1\", 2: \"comp6_rate_percent_diff_2\"})\n",
    "# df_train = pd.concat([df_train, df_dummy17], axis = 1)\n",
    "\n",
    "# # comp7_rate \n",
    "# dummy18 = pd.get_dummies(df_train['comp7_rate'])\n",
    "# df_dummy18 = dummy18.rename(columns = {(-1): \"comp7_rate_-1\", 0: \"comp7_rate_0\", 1: \"comp7_rate_1\", 2: \"comp7_rate_2\"})\n",
    "# df_train = pd.concat([df_train, df_dummy18], axis = 1)\n",
    "\n",
    "# # comp7_inv   \n",
    "# dummy19 = pd.get_dummies(df_train['comp7_inv'])\n",
    "# df_dummy19 = dummy19.rename(columns = {(-1): \"comp7_inv_-1\", 0: \"comp7_inv_0\", 1: \"comp7_inv_1\", 2: \"comp7_inv_2\"})\n",
    "# df_train = pd.concat([df_train, df_dummy19], axis = 1)\n",
    "\n",
    "# # comp7_rate_percent_diff     \n",
    "# dummy20 = pd.get_dummies(df_train['comp7_rate_percent_diff'])\n",
    "# df_dummy20 = dummy20.rename(columns = {(-1): \"comp7_rate_percent_diff_-1\", 0: \"comp7_rate_percent_diff_0\", 1: \"comp7_rate_percent_diff_1\", 2: \"comp7_rate_percent_diff_2\"})\n",
    "# df_train = pd.concat([df_train, df_dummy20], axis = 1)\n",
    "\n",
    "# # comp8_rate  \n",
    "# dummy21 = pd.get_dummies(df_train['comp8_rate'])\n",
    "# df_dummy21 = dummy21.rename(columns = {(-1): \"comp8_rate_-1\", 0: \"comp8_rate_0\", 1: \"comp8_rate_1\", 2: \"comp8_rate_2\"})\n",
    "# df_train = pd.concat([df_train, df_dummy21], axis = 1)\n",
    "\n",
    "# # comp8_inv\n",
    "# dummy22 = pd.get_dummies(df_train['comp8_inv'])\n",
    "# df_dummy22 = dummy22.rename(columns = {(-1): \"comp8_inv_-1\", 0: \"comp8_inv_0\", 1: \"comp8_inv_1\", 2: \"comp8_inv_2\"})\n",
    "# df_train = pd.concat([df_train, df_dummy22], axis = 1)\n",
    "\n",
    "# # comp8_rate_percent_diff \n",
    "# dummy23 = pd.get_dummies(df_train['comp8_rate_percent_diff'])\n",
    "# df_dummy23 = dummy23.rename(columns = {(-1): \"comp8_rate_percent_diff_-1\", 0: \"comp8_rate_percent_diff_0\", 1: \"comp8_rate_percent_diff_1\", 2: \"comp8_rate_percent_diff_2\"})\n",
    "# df_train = pd.concat([df_train, df_dummy23], axis = 1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def make_dummy(feature, name):\n",
    "#     dummy1 = pd.get_dummies(feature)\n",
    "#     df_dummy1 = dummy1.rename(columns = {-1: name + \"_-1\", 0: name+\"_0\", 1: name + \"_1\", 2: name +\"_2\"})\n",
    "#     df_train = pd.concat([df_train, df_dummy1], axis = 1)\n",
    "#     df_train = df_train.drop(feature)\n",
    "#     return df_train\n",
    "\n",
    "# dummies = [\"comp1_rate_percent_diff\",  \"comp2_rate\", \"comp2_inv\", \"comp2_rate_percent_diff\", \"comp3_rate\", \"comp3_inv\",\n",
    "#            \"comp3_rate_percent_diff\", \"comp4_rate\", \"comp4_inv\", \"comp4_rate_percent_diff\", \"comp5_rate\", \"comp5_inv\",\n",
    "#            \"comp5_rate_percent_diff\"]\n",
    "\n",
    "\n",
    "# dummies = [\"comp1_rate_percent_diff\"]\n",
    "\n",
    "# for i in dummies:\n",
    "#     make_dummy(df_train[i], i)\n",
    "\n",
    "# df_dummy1\n",
    "# df_train4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c79d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89c1ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348b7129",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy[dummy[-1] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eec4fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.comp1_rate.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4951cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[df_train[]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f549953",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18722c94-83eb-4fb0-9107-f8d2193e9438",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
